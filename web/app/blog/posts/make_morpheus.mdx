---
title: 'How language models work'
publishedAt: '2025-04-17'
---

I think language models are on of the coolest inventions of the century.

Most sci-fi doesnt even dream of AI being easy to talk to and aligned with humans best interests.

First we convert a sentence to an embedding, which is a list of numbers.

 Attention mechanisms let the model focus on different parts of a sentence, or even previous sentences, when deciding what to say next.

 # Attention is all you need

  "Attention Is All You Need" is a landmark 2017 research paper by Vaswani et al. from Google, which introduced the Transformer model—a revolutionary architecture for handling sequential data, particularly natural language (like sentences).

Here’s a plain-English breakdown:

1. What Problem Did It Solve?
Before Transformers, most AI language models relied on RNNs (Recurrent Neural Networks) or CNNs (Convolutional Neural Networks) for sequence data. These models struggled with:

Long-range dependencies (remembering words far apart in a sentence)

Slow training (can’t parallelize easily)

Hard to scale

2. What Did the Paper Propose?
The paper proposed to throw out recurrence and convolution entirely and use a mechanism called attention—specifically, self-attention.

The core idea is:

Instead of processing words one-by-one in order, the model looks at all words at once and decides which other words in the sentence are important for understanding each word.

This allows for parallel processing (fast training), and makes it easy for the model to “connect the dots” between distant words.

3. What is "Attention"?
Attention is a way for the model to focus on relevant parts of the input when making predictions.

For each word, it assigns a score to every other word in the sentence: How much should I pay attention to this other word?

It then combines information from all other words, weighted by these scores.

Self-attention: The model pays attention to all parts of the input sentence to understand each word in context.

4. Key Components
Multi-Head Self-Attention: Multiple "heads" look at different types of relationships in parallel.

Positional Encoding: Since the model sees all words at once, it needs a way to know their order (otherwise "the dog bit the man" and "the man bit the dog" look the same!).

Feed-Forward Layers: After attention, each word's representation is passed through a simple neural network.

Stacked Layers: The above are stacked multiple times for greater expressive power.

5. Why Was This Such a Big Deal?
Much faster training (parallelization)

Much better at handling long sentences

Easily scaled up to massive datasets

Became the foundation for nearly all modern AI language models (BERT, GPT, T5, etc.)

6. One-Liner Summary
The Transformer model shows that a neural network architecture relying only on attention mechanisms (and not recurrence or convolutions) can achieve state-of-the-art results in language tasks—hence, “Attention is All You Need.”

Bonus: Diagram (text version)